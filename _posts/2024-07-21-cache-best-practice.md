---
title: "缓存最佳实践总结"
date: 2024-07-21
---

# 前言
缓存在互联网在线服务中随处可见，因此有必要了解最佳实践。
参与过面试的朋友应该对缓存出现的这三种问题不陌生。（1）缓存穿透，（2）缓存击穿，（3）缓存雪崩。对解决方案也都倒背如流。
但这就是缓存所面临的全部问题了吗？远远不够。


下面是我总结出的几类问题以及对应的实践。了解这些问题，有助于根据业务得到最佳实践。

# 第一类：性能问题
1. 单节点问题？
2. 遇见Redis 性能瓶颈怎么办？
   1. hotkey 问题
   2. bigkey 问题

## 单节点问题？
redis提供了几种模式，用于不同的场景。（1）单实例,（2）redis sentinel, (3) master-slave, (4)redis cluster
选用正确的模式，例如 测试环境，可以只使用单实例，但线上至少需要主从。避免出现单点问题。
同时根据你的数据量，数据重要程度，选择相对应的内存规格，CPU 核数等等。
例如：我所在的项目，使用的 Redis-Cluster 集群，并且带有 proxy 节点，也就是 proxy 节点相当于smart client，而服务就像使用单节点一样使用 Redis Cluster。
![redis-cluster-in-biz](/images/redis-cluster-in-biz.png)

注意：公司内部使用通常会做微调，用来更好的适配业务，例如这里就是客户端不用感知分片节点信息。
而好处就是，扩容（缩容）方便，只有proxy 节点需要知道，喔，节点数量变了，分片信息变了！
而这些对应用都是无感知的。

而即使是 Redis Cluster，也不能很好的解决下面的几个问题。

## 遇见Redis 性能瓶颈怎么办？

### hotkey 怎么办？
赶上大促时，流量非常高，大家都在刷一个页面，抢一个商品。
对应到缓存上，就是单个 key 的流量很高。高到打爆了这个 Redis 分片节点。

从网上搜集 hotkey的资料：
- 京东的hotkey 项目：[HOTKEY](https://gitee.com/jd-platform-opensource/hotkey)
- hotkey 拆分成多个 key

京东hotkey 的项目思路是说，平台上我不知道哪个商品是热点，所以我需要探测热点，探测出来后，通知业务系统，这个 key 是热点的。至于业务系统是拆分还是直接放到本地缓存它都不关心。

第二个是你知道了是 hotkey 之后的处理。

不存在优劣之分，主要还是看业务场景。

不过这里我们也得到了 hotkey 的处理思路：1、探测hotkey；2、处理 hotkey 的请求。

#### 探测 hotkey
京东hotkey 项目是用一个独立的系统嵌入在应用中去统计 hotkey。再看第一张图，所有的流量都会经过 proxy 节点，那么就可以通过 proxy 节点去合并统计。这就是让集群具备了探测的能力。

1. 不需要维护额外的一套系统。
2. 全局性。

这也会衍生出更多的问题：（1）hotkey 的探测是否可以配置，hotkey 是否支持自定义探测函数等等。

所以实际上还是 应用中的client 去统计，然后 proxy 节点汇总统计，那当知道某个 key 是 hotkey 后，应该怎么做？

#### 处理 hotkey的请求
如果请求处理路径越短，那么意味着更少的网络耗时，更少的网络流量。
现在一个请求经过了application->proxy->redis 分片节点。
![cache-key-rquest-path](/images/cache-key-rquest-path.png)

也就是三个节点。这三个节点分别可以承接 hotkey 的流量。
- application 节点：可以用Local Cache ，那么当请求命中 Local Cache 后就可以直接返回，如果未命中，则可以回源去请求 Redis。

这个是不是最完美？当然不是，按下葫芦浮起瓢。使用 Local Cache 还带来了其他问题。（1）Local Cache 的数据一致性问题，当 Redis 中的数据被更新后，何时更新到 Local Cache 中呢？

- proxy 节点

可以把 hotkey 复制到 proxy 节点上， 这样请求在经过 proxy 节点时，如果是 hotkey 就可以直接返回给客户端了。带来的问题，proxy 节点复制数据了。
当然，实际上不会这么做，proxy 节点做太多事情会导致更大的问题，例如 proxy 过载，会导致整个 Redis 集群服务不可用。

- Redis 分片节点

单个 key 一定会落在一个分片实例上，如果能把 key 的请求分散在不同的实例上，就能解决性能问题。
例如 key_item_1 是 hotkey，如果把key_item_1拆分成 key_item_1_copy1,key_item_1_copy2,key_item_1_copy3,key_item_1_copy4。
四个 key。并且让这四个 key 分散在不同的实例上。
那么就可以支持4 倍的流量。
新的问题：（1） 谁来复制 hotkey，复制失败怎么办？（2）发生数据变更时，需要同时更新原始的 key，以及复制的 key，如何保证更新一定成功？

### bigkey问题

影响 redis 延迟的不仅仅是请求量，还有可能是网络 IO，带宽。
例如 bigkey，这个 key 里面的 value 很大，客户端虽然可以忍受一会儿延迟才能拿到数据。
但其他客户端可等不了这么久。
而延迟带来的可能是大量访问数据库重新更新缓存，从而引发了缓存雪崩问题。也可能是页面白屏，用户失望，关掉 APP，打开了竞对的网站。

解决此问题的方案是，不要设置大 key！！
这不是在危言耸听，因为在线服务对延迟敏感，而bigkey带来的延迟是不可控的。

另一种能缓解的措施是，将数据放在离用户更近的地方。例如，proxy 上，或者 Local Cache 中。

# 第二类：数据不一致
只要你使用了 Cache 就会带来数据不一致的风险，这是由于 Cache 本质上属于衍生数据，是通过将数据库的数据同步过来的。

而同步本身就会有延迟，不一致。再加上转换，处理逻辑失败等等，加剧了不一致的概率。

所以我们要小心的处理同步过程，不仅需要增量同步，更需要一个全量的 backup 的策略，能在数据出现问题时，执行 backup 的策略，来快速恢复数据。
例如：消息队列的重置消费 offset，Redis 的快照等等。

## MYSQL 和 Redis 的不一致
如果没有业务逻辑处理而直接将数据写入 Redis，那么会好办一些，因为 MYSQL 有 Binlog，而支持 Binlog 的数据管道也有很多。使用这些成熟的工具能避免踩坑。

如果有业务逻辑处理。那么请注意，在除了支持增量消费以外，最好再单独写一个全力同步的脚本。肯定会有用到的时候，这个时候你会庆幸自己没有偷懒。
我曾遇到的问题：
1. Binlog 中断，数据库发出来的 Binlog，会发生给另外一个管道，业务接收的这个管道的数据，而在某天，这个管道任务挂掉了，直到业务出现报警。
2. Binlog 延迟，由于架构迁移需要刷大量数据，大量数据的更新导致了 Binlog 延迟4-5 个小时，你会在监控图上看见lag线逐渐从一个水平线变成斜向上。
3. Redis 集群 内存不足。某次项目上线，新增了一个 key，不巧的是 key 写错了，包含一个指针，而 key 过期时间是 24 小时。此时 Redis 发出了内存使用率超过 99%的告警⚠️。

## 通用的缓存更新逻辑导致的不一致
缓存更新策略我这里就不多说了。
看看大佬的博客就明白了。
- [左耳朵耗子的文章：缓存更新的套路](https://coolshell.cn/articles/17416.html)
- [pdai-tech 中的缓存问题](https://www.pdai.tech/md/db/nosql-redis/db-redis-x-cache.html#%E6%95%B0%E6%8D%AE%E5%BA%93%E5%92%8C%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7)

## 多层 Cache 之间的缓存不一致（Local Cache 与 Redis Cache）

这个问题也很好理解，Local Cache 中没有数据，从 Redis 中将数据加载过来。
等待一个固定时间T后，Local Cache 过期，再次从Redis 中加载数据。
 而在这个时间范围内，RedisCache 如果有更新，那么 Local Cache 中的数据就不是最新的了。

如何解决这个问题呢？
看到这里你可能会发现这不就是 MySQL 和 Redis 的数据一致性问题的翻版么。
确实是这样。
所以也就明白了，没有完美的一致性方案。

考虑你的业务的优先级，再进行抉择。

这里提供几种备选的方案。
### 更短的过期时间
这样就能支持更快过期，更加快速的同步数据了。
这也是我正在使用的方案。

但是到此为止了吗？

### 监听变更
如果某个数据有变更，首先会更新到 MySQL 中，然后是 Redis，然后通过某次请求加载到 local Cache 中。
#### 监听 Binlog
是否可以在应用中监听MySQL 的变更，直接同步到 local Cache 中，并且应用一般是集群部署，而 local Cache 分布在每一台实例上。所以还需要广播到每台实例上。

可行吗？ 可行！代价较大，因为业务服务现在又增加了监听。

还有没有更轻量级的。
#### Redis watch key
Redis 允许 watch key的变更，这样是一个更轻量级的方案。我并没有实际上使用这个方案，不确定其中带来的风险，需要做好调研后使用。


# 第三类：Cache 命中率&缓存问题应对

大公司内多级缓存一般会封装成通用中间件，我曾经为业务实现过一个基本二级缓存中间件，只支持 GET& MGET 命令。

除了上面说到的不一致问题外，local Cache 我们要尽量提高其命中率，同时还要保持缓存的数据是最新的。

典型的既要又要！

但一个固定的过期时间显然不能满足上述要求，开动你的小脑瓜，想想有没有其他可能。

## 软过期（soft expired time）
在 localcache 中增加一个软过期时间，当其过期后，重新从 Redis 加载数据。当前请求使用local cache 的数据。
来分析一下：
1. local cache数据没有真的过期，请求可以使用。缓存命中率提高了。
2. 软过期实现了更快的更新 local cache。
3. 如果实现为异步，则不会阻塞当前的请求。
4. 只有真正过期后，才会从 local cache 中删除。
5. 保护了后端数据源，Redis，DB等。


而软过期时间实际上也是一个固定的时间点，例如 10 秒后，那么 10 秒后，所有的实例相关的请求都会去回源请求后端数据源。举个例子：

key:item_1现在软过期了，请求 item_1的请求每秒 10 万,此时发起回源的请求就有 10 万，这很明显不合理，很容易就把MySQL，Redis数据源打垮了。
你说单机 Redis 就能撑 10 万，但我的业务又不止这一个 key，是吧。

这个问题就是惊群效应。
## 惊群效应
就是说同一时间触发了超多的回源请求，造成了 Redis，MySQL 的过载。
在Redis，MySQL上可以设置限流方案，但是不能把鸡蛋都放在一个篮子里。
有空补充图，更形象直观。

业务系统也需要控制速率，简单的办法就是加锁。
### 实例内加锁
保证这个实例下只有一个请求能到达 DB，那在应用集群实例不太多的时候是可以的。
### 加分布式锁
保证集群内相同的请求只有一个能到达 DB，完美。再加上集群内广播的方式将更新后的 key 更新到所有实例上。

# 总结
没有银弹，根据业务特性，权衡利弊后，选择最合适的方案。
